login as: balakrba
Keyboard-interactive authentication prompts from server:
| Password:
End of keyboard-interactive prompts from server
balakrba@hadoop-gate-0:~$ pyspark
Python 2.7.12 (default, Oct  8 2019, 14:14:10)
[GCC 5.4.0 20160609] on linux2
Type "help", "copyright", "credits" or "license" for more information.
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 2.3.2.3.1.0.0-78
      /_/

Using Python version 2.7.12 (default, Oct  8 2019 14:14:10)
SparkSession available as 'spark'.
>>> file_path1 = "/data/weather/2010"
>>> file_path2 = "/data/weather/2011"
>>> file_path3 = "/data/weather/2012"
>>> file_path4 = "/data/weather/2013"
>>> file_path5 = "/data/weather/2014"
>>> file_path6 = "/data/weather/2015"
>>> inTextData1 = spark.read.format("csv").option("header", "true").option("delimiter","\t").load(file_path1)
>>> inTextData2 = spark.read.format("csv").option("header", "true").option("delimiter","\t").load(file_path2)
20/04/13 01:52:01 WARN SharedInMemoryCache: Evicting cached table partition metadata from memory due to size constraints (spark.sql.hive.filesourcePartitionFileCacheSize = 262144000 bytes). This may impact query planning performance.
>>> inTextData3 = spark.read.format("csv").option("header", "true").option("delimiter","\t").load(file_path3)
>>> inTextData4 = spark.read.format("csv").option("header", "true").option("delimiter","\t").load(file_path4)
>>> inTextData5 = spark.read.format("csv").option("header", "true").option("delimiter","\t").load(file_path5)
>>> inTextData6 = spark.read.format("csv").option("header", "true").option("delimiter","\t").load(file_path6)
>>> inTextData = inTextData1.union(inTextData2).union(inTextData3).union(inTextData4).union(inTextData5).union(inTextData6).distinct()
>>> def dropColumns(x):
...     columns = x.split(" ")
...     output = columns[0] + " " + columns[2] + " " + columns[3] + " " + columns[5] + " " + columns[7] + " " + columns[
...         9] + " " + columns[11] + " " + columns[13] + " " + columns[15] + " " + columns[16] + " " + columns[17] + " " + \
...              columns[18] + " " + columns[19] + " " + columns[20] + " " + columns[21]
...     return output
...
>>> def cleaningdata(x,name):
...     from pyspark.sql.functions import col, split
...     rdd1 = x.rdd
...     rdd2 = rdd1.map(lambda x: str(x).split('=')[1])
...     rdd3 = rdd2.map(lambda x: ' '.join(x.split()))
...     rdd4 = rdd3.map(lambda x: x[1:-2])
...     rdd5 = rdd4.map(lambda x: x.replace("'", ""))
...     rdd6 = rdd5.map(dropColumns)
...     file_store = "/user/balakrba/"+name
...     rdd6.saveAsTextFile(file_store)
...     newInData = spark.read.csv(file_store , header=False, sep=' ')
...     cleanData1 = newInData.withColumnRenamed('_c0', 'STN').withColumnRenamed('_c1', 'YEARMODA') \
...         .withColumnRenamed('_c2', 'TEMP').withColumnRenamed('_c3', 'DEWP') \
...         .withColumnRenamed('_c4', 'SLP').withColumnRenamed('_c5', 'STP') \
...         .withColumnRenamed('_c6', 'VISIB').withColumnRenamed('_c7', 'WDSP') \
...         .withColumnRenamed('_c8', 'MXSPD').withColumnRenamed('_c9', 'GUST') \
...         .withColumnRenamed('_c10', 'MAX').withColumnRenamed('_c11', 'MIN') \
...         .withColumnRenamed('_c12', 'PRCP').withColumnRenamed('_c13', 'SNDP') \
...         .withColumnRenamed('_c14', 'FRSHTT')
...     cleanData2 = cleanData1.withColumn("MIN", split(col("MIN"), "\\*").getItem(0))
...     cleanData3 = cleanData2.withColumn("MAX", split(col("MAX"), "\\*").getItem(0))
...     cleanData3 = cleanData3.withColumn("MAX_NEW", cleanData3["MAX"].cast("double"))
...     cleanData3 = cleanData3.withColumn("MIN_NEW", cleanData3["MIN"].cast("double"))
...     cleanData3 = cleanData3.drop('MAX', 'MIN')
...     cleanData3 = cleanData3.withColumnRenamed("MAX_NEW", "MAX")
...     cleanData3 = cleanData3.withColumnRenamed("MIN_NEW", "MIN")
...     cleanData4 = cleanData3.filter(cleanData3.MAX <> 9999.9)
...     cleanData4 = cleanData4.filter(cleanData4.MIN <> 9999.9)
...     return cleanData4
...
>>> cleaned = cleaningdata(inTextData,"part1")
>>> cleaned.write.csv("/user/balakrba/dataall_2010")
login as: balakrba
Keyboard-interactive authentication prompts from server:
| Password:
End of keyboard-interactive prompts from server
balakrba@hadoop-gate-0:~$ pyspark
Python 2.7.12 (default, Oct  8 2019, 14:14:10)
[GCC 5.4.0 20160609] on linux2
Type "help", "copyright", "credits" or "license" for more information.
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 2.3.2.3.1.0.0-78
      /_/

Using Python version 2.7.12 (default, Oct  8 2019 14:14:10)
SparkSession available as 'spark'.
>>> file_path7 = "/data/weather/2016"
>>> file_path8 = "/data/weather/2017"
>>> file_path9 = "/data/weather/2018"
>>> file_path10 = "/data/weather/2019"
>>> inTextData7 = spark.read.format("csv").option("header", "true").option("delimiter","\t").load(file_path7)
>>> inTextData8 = spark.read.format("csv").option("header", "true").option("delimiter","\t").load(file_path8)
20/04/13 02:27:01 WARN SharedInMemoryCache: Evicting cached table partition metadata from memory due to size constraints (spark.sql.hive.filesourcePartitionFileCacheSize = 262144000 bytes). This may impact query planning performance.
>>> inTextData9 = spark.read.format("csv").option("header", "true").option("delimiter","\t").load(file_path9)
>>> inTextData10 = spark.read.format("csv").option("header", "true").option("delimiter","\t").load(file_path10)
>>> inTextData = inTextData7.union(inTextData8).union(inTextData9).union(inTextData10).distinct()
>>> def dropColumns(x):
...     columns = x.split(" ")
...     output = columns[0] + " " + columns[2] + " " + columns[3] + " " + columns[5] + " " + columns[7] + " " + columns[
...         9] + " " + columns[11] + " " + columns[13] + " " + columns[15] + " " + columns[16] + " " + columns[17] + " " + \
...              columns[18] + " " + columns[19] + " " + columns[20] + " " + columns[21]
...     return output
...
>>> def cleaningdata(x,name):
...     from pyspark.sql.functions import col, split
...     rdd1 = x.rdd
...     rdd2 = rdd1.map(lambda x: str(x).split('=')[1])
...     rdd3 = rdd2.map(lambda x: ' '.join(x.split()))
...     rdd4 = rdd3.map(lambda x: x[1:-2])
...     rdd5 = rdd4.map(lambda x: x.replace("'", ""))
...     rdd6 = rdd5.map(dropColumns)
...     file_store = "/user/balakrba/"+name
...     rdd6.saveAsTextFile(file_store)
...     newInData = spark.read.csv(file_store , header=False, sep=' ')
...     cleanData1 = newInData.withColumnRenamed('_c0', 'STN').withColumnRenamed('_c1', 'YEARMODA') \
...         .withColumnRenamed('_c2', 'TEMP').withColumnRenamed('_c3', 'DEWP') \
...         .withColumnRenamed('_c4', 'SLP').withColumnRenamed('_c5', 'STP') \
...         .withColumnRenamed('_c6', 'VISIB').withColumnRenamed('_c7', 'WDSP') \
...         .withColumnRenamed('_c8', 'MXSPD').withColumnRenamed('_c9', 'GUST') \
...         .withColumnRenamed('_c10', 'MAX').withColumnRenamed('_c11', 'MIN') \
...         .withColumnRenamed('_c12', 'PRCP').withColumnRenamed('_c13', 'SNDP') \
...         .withColumnRenamed('_c14', 'FRSHTT')
...     cleanData2 = cleanData1.withColumn("MIN", split(col("MIN"), "\\*").getItem(0))
...     cleanData3 = cleanData2.withColumn("MAX", split(col("MAX"), "\\*").getItem(0))
...     cleanData3 = cleanData3.withColumn("MAX_NEW", cleanData3["MAX"].cast("double"))
...     cleanData3 = cleanData3.withColumn("MIN_NEW", cleanData3["MIN"].cast("double"))
...     cleanData3 = cleanData3.drop('MAX', 'MIN')
...     cleanData3 = cleanData3.withColumnRenamed("MAX_NEW", "MAX")
...     cleanData3 = cleanData3.withColumnRenamed("MIN_NEW", "MIN")
...     cleanData4 = cleanData3.filter(cleanData3.MAX <> 9999.9)
...     cleanData4 = cleanData4.filter(cleanData4.MIN <> 9999.9)
...     return cleanData4
...
>>> inTextData = inTextData7.union(inTextData8).union(inTextData9).union(inTextData10).distinct()
>>> cleaned = cleaningdata(inTextData,"part2")
>>> cleaned.write.csv("/user/balakrba/dataall_2016")
>>>login as: balakrba
Keyboard-interactive authentication prompts from server:
| Password:
End of keyboard-interactive prompts from server
balakrba@hadoop-gate-0:~$ hdfs dfs -ls /user/balakrba/
Found 8 items
drwxr-xr-x   - balakrba hdfs          0 2020-04-13 02:24 /user/balakrba/\usealakrba\cleaned_1
drwx------   - balakrba hdfs          0 2020-04-13 08:00 /user/balakrba/.Trash
drwxr-xr-x   - balakrba hdfs          0 2020-04-13 03:41 /user/balakrba/.sparkStaging
drwxr-xr-x   - balakrba hdfs          0 2020-04-13 03:39 /user/balakrba/dataall_2010
drwxr-xr-x   - balakrba hdfs          0 2020-04-13 02:56 /user/balakrba/dataall_2016
drwxr-xr-x   - balakrba hdfs          0 2020-04-13 03:35 /user/balakrba/part1_
drwxr-xr-x   - balakrba hdfs          0 2020-04-13 02:42 /user/balakrba/part2
-rw-r--r--   3 balakrba hdfs        134 2020-04-07 21:43 /user/balakrba/purplecow.txt
balakrba@hadoop-gate-0:~$ pyspark
Python 2.7.12 (default, Oct  8 2019, 14:14:10)
[GCC 5.4.0 20160609] on linux2
Type "help", "copyright", "credits" or "license" for more information.
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
20/04/13 12:51:07 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
20/04/13 12:51:07 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
20/04/13 12:51:07 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
20/04/13 12:51:07 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
20/04/13 12:51:07 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
20/04/13 12:51:07 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
20/04/13 12:51:07 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.
20/04/13 12:51:07 WARN Utils: Service 'SparkUI' could not bind on port 4047. Attempting port 4048.
20/04/13 12:51:07 WARN Utils: Service 'SparkUI' could not bind on port 4048. Attempting port 4049.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 2.3.2.3.1.0.0-78
      /_/

Using Python version 2.7.12 (default, Oct  8 2019 14:14:10)
SparkSession available as 'spark'.
>>> filestore_1 = "/user/balakrba/dataall_2010"
>>> filestore_2 = "/user/balakrba/dataall_2016"
>>> newInData = spark.read.csv(filestore_1 , header=False, sep=' ')
>>> newInData1 = spark.read.csv(filestore_2 , header=False, sep=' ')
>>> final_Data = newInData.union(newInData1).distinct()
>>> cleanData = final_Data.withColumnRenamed('_c0', 'STN').withColumnRenamed('_c1', 'YEARMODA') \
...     .withColumnRenamed('_c2', 'TEMP').withColumnRenamed('_c3', 'DEWP') \
...     .withColumnRenamed('_c4', 'SLP').withColumnRenamed('_c5', 'STP') \
...     .withColumnRenamed('_c6', 'VISIB').withColumnRenamed('_c7', 'WDSP') \
...     .withColumnRenamed('_c8', 'MXSPD').withColumnRenamed('_c9', 'GUST') \
...     .withColumnRenamed('_c10', 'PRCP').withColumnRenamed('_c11', 'SNDP') \
...     .withColumnRenamed('_c12', 'FRSHTT').withColumnRenamed('_c13', 'MAX') \
...     .withColumnRenamed('_c14', 'MIN')
>>> cleanData.createOrReplaceTempView("Data_All")
>>> cleanData = cleanData.withColumn("MAX_NEW", cleanData["MAX"].cast("double"))
>>> cleanData = cleanData.withColumn("MIN_NEW", cleanData["MIN"].cast("double"))
>>> cleanData = cleanData.drop('MAX', 'MIN')
>>> cleanData = cleanData.withColumnRenamed("MAX_NEW", "MAX")
>>> cleanData = cleanData.withColumnRenamed("MIN_NEW", "MIN")
>>> cleanData.createOrReplaceTempView("Data_All")
>>> myResult = spark.sql("SELECT distinct(STN), LEFT(YEARMODA,4) as YEAR, RIGHT(LEFT(YEARMODA,6),2) as MONTH, RIGHT(YEARMODA,2) as DATE, MAX from Data_All where Max in (Select MAX(MAX) from Data_All where Max <>9999.9 and LEFT(YEARMODA,4)='2010') and LEFT(YEARMODA,4) ='2010' ").show()
Hive Session ID = 5e1db4ef-0146-4285-9534-99102ff71349
20/04/13 13:22:17 ERROR JniBasedUnixGroupsMapping: error looking up the name of group 1013506318: No such file or directory
+------+----+-----+----+-----+
|   STN|YEAR|MONTH|DATE|  MAX|
+------+----+-----+----+-----+
|720667|2010|   09|  23|132.8|
+------+----+-----+----+-----+
pyspark.sql.utils.AnalysisException: u'Table or view not found: cleanData; line 1 pos 120'
>>> myResult = spark.sql("SELECT distinct(STN), LEFT(YEARMODA,4) as YEAR, RIGHT(LEFT(YEARMODA,6),2) as MONTH, RIGHT(YEARMODA,2) as DATE, MAX from Data_All where Max in (Select MAX(MAX) from Data_All where Max <>9999.9 and LEFT(YEARMODA,4)='2010') and LEFT(YEARMODA,4) ='2010' ").show()
+------+----+-----+----+-----+
|   STN|YEAR|MONTH|DATE|  MAX|
+------+----+-----+----+-----+
|720667|2010|   09|  23|132.8|
+------+----+-----+----+-----+

>>> spark.sql("SELECT distinct(STN), LEFT(YEARMODA,4) as YEAR, RIGHT(LEFT(YEARMODA,6),2) as MONTH, RIGHT(YEARMODA,2) as DATE, MAX from Data_All where Max in (Select MAX(MAX) from Data_All where Max <>9999.9 and LEFT(YEARMODA,4)='2011') and LEFT(YEARMODA,4) ='2011' ").show()
+------+----+-----+----+-----+
|   STN|YEAR|MONTH|DATE|  MAX|
+------+----+-----+----+-----+
|406355|2011|   08|  03|127.4|
+------+----+-----+----+-----+

>>> spark.sql("SELECT distinct(STN), LEFT(YEARMODA,4) as YEAR, RIGHT(LEFT(YEARMODA,6),2) as MONTH, RIGHT(YEARMODA,2) as DATE, MAX from Data_All where Max in (Select MAX(MAX) from Data_All where Max <>9999.9 and LEFT(YEARMODA,4)='2012') and LEFT(YEARMODA,4) ='2012' ").show()
+------+----+-----+----+-----+
|   STN|YEAR|MONTH|DATE|  MAX|
+------+----+-----+----+-----+
|722577|2012|   07|  12|132.8|
+------+----+-----+----+-----+

>>> spark.sql("SELECT distinct(STN), LEFT(YEARMODA,4) as YEAR, RIGHT(LEFT(YEARMODA,6),2) as MONTH, RIGHT(YEARMODA,2) as DATE, MAX from Data_All where Max in (Select MAX(MAX) from Data_All where Max <>9999.9 and LEFT(YEARMODA,4)='2013') and LEFT(YEARMODA,4) ='2013' ").show()
+------+----+-----+----+-----+
|   STN|YEAR|MONTH|DATE|  MAX|
+------+----+-----+----+-----+
|406890|2013|   07|  12|132.8|
+------+----+-----+----+-----+

>>> spark.sql("SELECT distinct(STN), LEFT(YEARMODA,4) as YEAR, RIGHT(LEFT(YEARMODA,6),2) as MONTH, RIGHT(YEARMODA,2) as DATE, MAX from Data_All where Max in (Select MAX(MAX) from Data_All where Max <>9999.9 and LEFT(YEARMODA,4)='2014') and LEFT(YEARMODA,4) ='2014' ").show()
+------+----+-----+----+-----+
|   STN|YEAR|MONTH|DATE|  MAX|
+------+----+-----+----+-----+
|406650|2014|   08|  03|129.6|
+------+----+-----+----+-----+

>>> spark.sql("SELECT distinct(STN), LEFT(YEARMODA,4) as YEAR, RIGHT(LEFT(YEARMODA,6),2) as MONTH, RIGHT(YEARMODA,2) as DATE, MAX from Data_All where Max in (Select MAX(MAX) from Data_All where Max <>9999.9 and LEFT(YEARMODA,4)='2015') and LEFT(YEARMODA,4) ='2015' ").show()
+------+----+-----+----+-----+
|   STN|YEAR|MONTH|DATE|  MAX|
+------+----+-----+----+-----+
|916700|2015|   10|  21|132.4|
+------+----+-----+----+-----+

>>> spark.sql("SELECT distinct(STN), LEFT(YEARMODA,4) as YEAR, RIGHT(LEFT(YEARMODA,6),2) as MONTH, RIGHT(YEARMODA,2) as DATE, MAX from Data_All where Max in (Select MAX(MAX) from Data_All where Max <>9999.9 and LEFT(YEARMODA,4)='2016') and LEFT(YEARMODA,4) ='2016' ").show()
+------+----+-----+----+-----+
|   STN|YEAR|MONTH|DATE|  MAX|
+------+----+-----+----+-----+
|700638|2016|   06|  22|129.0|
+------+----+-----+----+-----+

>>> spark.sql("SELECT distinct(STN), LEFT(YEARMODA,4) as YEAR, RIGHT(LEFT(YEARMODA,6),2) as MONTH, RIGHT(YEARMODA,2) as DATE, MAX from Data_All where Max in (Select MAX(MAX) from Data_All where Max <>9999.9 and LEFT(YEARMODA,4)='2017') and LEFT(YEARMODA,4) ='2017' ").show()
+------+----+-----+----+-----+
|   STN|YEAR|MONTH|DATE|  MAX|
+------+----+-----+----+-----+
|917430|2017|   04|  10|129.6|
+------+----+-----+----+-----+

>>> spark.sql("SELECT distinct(STN), LEFT(YEARMODA,4) as YEAR, RIGHT(LEFT(YEARMODA,6),2) as MONTH, RIGHT(YEARMODA,2) as DATE, MAX from Data_All where Max in (Select MAX(MAX) from Data_All where Max <>9999.9 and LEFT(YEARMODA,4)='2018') and LEFT(YEARMODA,4) ='2018' ").show()
+------+----+-----+----+-----+
|   STN|YEAR|MONTH|DATE|  MAX|
+------+----+-----+----+-----+
|408110|2018|   07|  02|126.3|
+------+----+-----+----+-----+

>>> spark.sql("SELECT distinct(STN), LEFT(YEARMODA,4) as YEAR, RIGHT(LEFT(YEARMODA,6),2) as MONTH, RIGHT(YEARMODA,2) as DATE, MAX from Data_All where Max in (Select MAX(MAX) from Data_All where Max <>9999.9 and LEFT(YEARMODA,4)='2019') and LEFT(YEARMODA,4) ='2019' ").show()
+------+----+-----+----+-----+
|   STN|YEAR|MONTH|DATE|  MAX|
+------+----+-----+----+-----+
|956660|2019|   01|  24|121.1|
+------+----+-----+----+-----+

>>> spark.sql("SELECT distinct(STN), LEFT(YEARMODA,4) as YEAR, RIGHT(LEFT(YEARMODA,6),2) as MONTH, RIGHT(YEARMODA,2) as DATE, MIN from Data_All where Min in (Select MIN(MIN) from Data_All where Min <>9999.9 and LEFT(YEARMODA,4)='2010') and LEFT(YEARMODA,4) ='2010' ").show()
+------+----+-----+----+------+
|   STN|YEAR|MONTH|DATE|   MIN|
+------+----+-----+----+------+
|896060|2010|   08|  02|-115.2|
+------+----+-----+----+------+

>>> spark.sql("SELECT distinct(STN), LEFT(YEARMODA,4) as YEAR, RIGHT(LEFT(YEARMODA,6),2) as MONTH, RIGHT(YEARMODA,2) as DATE, MIN from Data_All where Min in (Select MIN(MIN) from Data_All where Min <>9999.9 and LEFT(YEARMODA,4)='2011') and LEFT(YEARMODA,4) ='2011' ").show()
+------+----+-----+----+------+
|   STN|YEAR|MONTH|DATE|   MIN|
+------+----+-----+----+------+
|897340|2011|   09|  17|-111.8|
+------+----+-----+----+------+

>>> spark.sql("SELECT distinct(STN), LEFT(YEARMODA,4) as YEAR, RIGHT(LEFT(YEARMODA,6),2) as MONTH, RIGHT(YEARMODA,2) as DATE, MIN from Data_All where Min in (Select MIN(MIN) from Data_All where Min <>9999.9 and LEFT(YEARMODA,4)='2012') and LEFT(YEARMODA,4) ='2012' ").show()
+------+----+-----+----+------+
|   STN|YEAR|MONTH|DATE|   MIN|
+------+----+-----+----+------+
|896060|2012|   09|  16|-119.6|
+------+----+-----+----+------+

>>> spark.sql("SELECT distinct(STN), LEFT(YEARMODA,4) as YEAR, RIGHT(LEFT(YEARMODA,6),2) as MONTH, RIGHT(YEARMODA,2) as DATE, MIN from Data_All where Min in (Select MIN(MIN) from Data_All where Min <>9999.9 and LEFT(YEARMODA,4)='2013') and LEFT(YEARMODA,4) ='2013' ").show()
+------+----+-----+----+------+
|   STN|YEAR|MONTH|DATE|   MIN|
+------+----+-----+----+------+
|895770|2013|   07|  31|-115.1|
|895770|2013|   07|  30|-115.1|
+------+----+-----+----+------+

>>> spark.sql("SELECT distinct(STN), LEFT(YEARMODA,4) as YEAR, RIGHT(LEFT(YEARMODA,6),2) as MONTH, RIGHT(YEARMODA,2) as DATE, MIN from Data_All where Min in (Select MIN(MIN) from Data_All where Min <>9999.9 and LEFT(YEARMODA,4)='2014') and LEFT(YEARMODA,4) ='2014' ").show()
+------+----+-----+----+------+
|   STN|YEAR|MONTH|DATE|   MIN|
+------+----+-----+----+------+
|896060|2014|   08|  21|-113.4|
+------+----+-----+----+------+

>>> spark.sql("SELECT distinct(STN), LEFT(YEARMODA,4) as YEAR, RIGHT(LEFT(YEARMODA,6),2) as MONTH, RIGHT(YEARMODA,2) as DATE, MIN from Data_All where Min in (Select MIN(MIN) from Data_All where Min <>9999.9 and LEFT(YEARMODA,4)='2015') and LEFT(YEARMODA,4) ='2015' ").show()
+------+----+-----+----+------+
|   STN|YEAR|MONTH|DATE|   MIN|
+------+----+-----+----+------+
|895770|2015|   09|  17|-114.2|
|896060|2015|   08|  22|-114.2|
+------+----+-----+----+------+

>>> spark.sql("SELECT distinct(STN), LEFT(YEARMODA,4) as YEAR, RIGHT(LEFT(YEARMODA,6),2) as MONTH, RIGHT(YEARMODA,2) as DATE, MIN from Data_All where Min in (Select MIN(MIN) from Data_All where Min <>9999.9 and LEFT(YEARMODA,4)='2016') and LEFT(YEARMODA,4) ='2016' ").show()
+------+----+-----+----+------+
|   STN|YEAR|MONTH|DATE|   MIN|
+------+----+-----+----+------+
|896060|2016|   07|  12|-115.1|
+------+----+-----+----+------+

>>> spark.sql("SELECT distinct(STN), LEFT(YEARMODA,4) as YEAR, RIGHT(LEFT(YEARMODA,6),2) as MONTH, RIGHT(YEARMODA,2) as DATE, MIN from Data_All where Min in (Select MIN(MIN) from Data_All where Min <>9999.9 and LEFT(YEARMODA,4)='2017') and LEFT(YEARMODA,4) ='2017' ").show()
+------+----+-----+----+------+
|   STN|YEAR|MONTH|DATE|   MIN|
+------+----+-----+----+------+
|896250|2017|   06|  20|-116.0|
+------+----+-----+----+------+

>>> spark.sql("SELECT distinct(STN), LEFT(YEARMODA,4) as YEAR, RIGHT(LEFT(YEARMODA,6),2) as MONTH, RIGHT(YEARMODA,2) as DATE, MIN from Data_All where Min in (Select MIN(MIN) from Data_All where Min <>9999.9 and LEFT(YEARMODA,4)='2018') and LEFT(YEARMODA,4) ='2018' ").show()
+------+----+-----+----+------+
|   STN|YEAR|MONTH|DATE|   MIN|
+------+----+-----+----+------+
|896060|2018|   08|  28|-116.3|
+------+----+-----+----+------+

>>> spark.sql("SELECT distinct(STN), LEFT(YEARMODA,4) as YEAR, RIGHT(LEFT(YEARMODA,6),2) as MONTH, RIGHT(YEARMODA,2) as DATE, MIN from Data_All where Min in (Select MIN(MIN) from Data_All where Min <>9999.9 and LEFT(YEARMODA,4)='2019') and LEFT(YEARMODA,4) ='2019' ").show()
+------+----+-----+----+------+
|   STN|YEAR|MONTH|DATE|   MIN|
+------+----+-----+----+------+
|896060|2019|   04|  05|-102.1|
+------+----+-----+----+------+

>>> spark.sql("SELECT distinct(STN), LEFT(YEARMODA,4) as YEAR, RIGHT(LEFT(YEARMODA,6),2) as MONTH, RIGHT(YEARMODA,2) as DATE, MAX from Data_All where MAX IN(SELECT MAX(MAX) FROM Data_All where MAX <> 9999.9) ").show()
+------+----+-----+----+-----+
|   STN|YEAR|MONTH|DATE|  MAX|
+------+----+-----+----+-----+
|720667|2010|   09|  23|132.8|
|406890|2013|   07|  12|132.8|
|722577|2012|   07|  12|132.8|
+------+----+-----+----+-----+

>>> spark.sql("SELECT distinct(STN), LEFT(YEARMODA,4) as YEAR, RIGHT(LEFT(YEARMODA,6),2) as MONTH, RIGHT(YEARMODA,2) as DATE, MIN from Data_All where MIN IN(SELECT MIN(MIN) FROM Data_All where MIN <> 9999.9) ").show()
+------+----+-----+----+------+
|   STN|YEAR|MONTH|DATE|   MIN|
+------+----+-----+----+------+
|896060|2012|   09|  16|-119.6|
+------+----+-----+----+------+


>>> spark.sql("Select STN,RIGHT(YEARMODA,2) as DATE, RIGHT(LEFT(YEARMODA,6),2) as MONTH, LEFT(YEARMODA,4) as YEAR, PRCP  from Data_All where PRCP in (SELECT MAX(PRCP) from Data_All where PRCP <>'99.99')").show()
+------+----+-----+----+-----+
|   STN|DATE|MONTH|YEAR| PRCP|
+------+----+-----+----+-----+
|720553|  01|   06|2017|9.99G|
|485010|  02|   10|2011|9.99G|
|722351|  05|   12|2016|9.99G|
+------+----+-----+----+-----+

>>> from pyspark.sql.functions import col, split
>>> cleanData=cleanData.withColumn("PRCP", split(col("PRCP"), "[A-Z]").getItem(0)).withColumn("P1", split(col("PRCP"), "[A-Z]").getItem(1))
>>> cleanData.createOrReplaceTempView("Data_All")
>>> spark.sql("Select STN,RIGHT(YEARMODA,2) as DATE, RIGHT(LEFT(YEARMODA,6),2) as MONTH, LEFT(YEARMODA,4) as YEAR, PRCP  from Data_All where PRCP in (SELECT MAX(PRCP) from Data_All where PRCP <>'99.99' and LEFT(YEARMODA,4)='2015') and LEFT(YEARMODA,4) ='2015' ").show()
+------+----+-----+----+----+
|   STN|DATE|MONTH|YEAR|PRCP|
+------+----+-----+----+----+
|983340|  16|   12|2015|9.92|
|418621|  21|   08|2015|9.92|
|645510|  23|   11|2015|9.92|
|419890|  24|   07|2015|9.92|
+------+----+-----+----+----+

>>> spark.sql("Select STN,RIGHT(YEARMODA,2) as DATE, RIGHT(LEFT(YEARMODA,6),2) as MONTH, LEFT(YEARMODA,4) as YEAR, PRCP  from Data_All where PRCP in (SELECT MIN(PRCP) from Data_All where PRCP <>'99.99' and LEFT(YEARMODA,4)='2015') and LEFT(YEARMODA,4) ='2015' ").show()
+------+----+-----+----+----+
|   STN|DATE|MONTH|YEAR|PRCP|
+------+----+-----+----+----+
|725090|  18|   10|2015|0.00|
|999999|  26|   01|2015|0.00|
|703210|  08|   09|2015|0.00|
|726593|  12|   05|2015|0.00|
|720912|  08|   01|2015|0.00|
|074380|  13|   02|2015|0.00|
|161490|  16|   11|2015|0.00|
|166421|  04|   10|2015|0.00|
|172650|  24|   12|2015|0.00|
|243430|  31|   05|2015|0.00|
|425910|  04|   08|2015|0.00|
|442880|  22|   09|2015|0.00|
|476750|  02|   05|2015|0.00|
|478000|  19|   08|2015|0.00|
|517090|  19|   03|2015|0.00|
|539590|  27|   05|2015|0.00|
|587540|  10|   01|2015|0.00|
|714400|  21|   05|2015|0.00|
|941740|  21|   08|2015|0.00|
|948500|  22|   10|2015|0.00|
+------+----+-----+----+----+
only showing top 20 rows

>>> missingdf=cleanData.where("STP='9999.9'")
>>> missingtotal=(float)(missingdf.count())
>>> total=(float)(cleanData.count())
>>> percent=(missingtotal*100)/(total)
>>> print(percent)
27.961149261
>>> spark.sql("Select STN,RIGHT(YEARMODA,2)as DATE, RIGHT(LEFT(YEARMODA,6),2) as MONTH,LEFT(YEARMODA,4) as YEAR,GUST from Data_All where GUST in (Select MAX(GUST) from Data_All where GUST <>'999.9' and LEFT(YEARMODA,4)='2019') and LEFT(YEARMODA,4)='2019'").show()
+------+----+----+----+
|   STN|DATE|YEAR|GUST|
+------+----+----+----+
|726130|  25|2019|99.1|
+------+----+----+----+

>>>


